# ClipCap-Mamba: Leveraging CLIP prefix to condition Mamba for image captioning

Final project for the **Natural Language Processing** course taught by profs. Stefano Faralli and Iacopo Masi at **Sapienza University of Rome** in A.Y. 2023/2024.

This study introduces a novel perspective by diverging from the conventional focus on attention-based models. Instead, it investigates the integration of the state-of-the-art state space model Mamba [[1](https://arxiv.org/abs/2312.00752)] with the ClipCap [[2](https://arxiv.org/abs/2111.09734)] framework.

For further details, you can refer to the [report](ClipCap_Mamba_Report.pdf).

### References

<a id="1">[1]</a> Gu, Albert, and Tri Dao. "Mamba: Linear-time sequence modeling with selective state spaces." arXiv preprint arXiv:2312.00752 (2023).\
<a id="1">[2]</a> Mokady, Ron, Amir Hertz, and Amit H. Bermano. "Clipcap: Clip prefix for image captioning." arXiv preprint arXiv:2111.09734 (2021).

## Author

[Chiara Ballanti](https://github.com/Ballants)
