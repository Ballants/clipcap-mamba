# ClipCap-Mamba: Leveraging CLIP prefix to condition Mamba for image captioning

Final project for the **Natural Language Processing** course taught by profs. Stefano Faralli and Iacopo Masi at **Sapienza University of Rome** in A.Y. 2023/2024.

This study introduces a novel perspective by diverging from the conventional focus on attention-based models. Instead, it investigates the integration of the state-of-the-art state space model Mamba [[1](https://arxiv.org/abs/2312.00752)] with the ClipCap [[2](https://arxiv.org/abs/2111.09734)] framework.
The model was trained on the Flickr30k [[3](https://web.archive.org/web/20180423084719id_/https://www.transacl.org/ojs/index.php/tacl/article/viewFile/229/33)] dataset.

For further details, you can refer to the [report](ClipCap_Mamba_Report.pdf).\
Examples of captioned images can be found [here](results_examples/Flickr30k).

### References

<a id="1">[1]</a> Gu, Albert, and Tri Dao. "Mamba: Linear-time sequence modeling with selective state spaces." arXiv preprint arXiv:2312.00752 (2023).\
<a id="2">[2]</a> Mokady, Ron, Amir Hertz, and Amit H. Bermano. "Clipcap: Clip prefix for image captioning." arXiv preprint arXiv:2111.09734 (2021).\
<a id="3">[3]</a> Young, Peter, et al. "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions." Transactions of the Association for Computational Linguistics 2 (2014): 67-78.
## Author

[Chiara Ballanti](https://github.com/Ballants)
